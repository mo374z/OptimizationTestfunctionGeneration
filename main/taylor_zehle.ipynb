{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taylor-Zehle Loss\n",
    "This notebook covers the simulation of a Function utilizing the self implemented Taylor-Zehle loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.utils import plot_simulated_meshgrid, plot_collage\n",
    "import model\n",
    "from utils import gt_sampling\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import bbobtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ground truth functions\n",
    "n_dim = 2\n",
    "samples = 1000\n",
    "seed = 42\n",
    "\n",
    "problem_f01 = bbobtorch.create_f01(n_dim, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from groundtruth functions\n",
    "sample_f01 = gt_sampling.get_sample(problem_f01, n_samples=samples, n_dim=2, seed=42, method='random', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = sample_f01[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the function with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_order_derivatives(f, wrt, n):\n",
    "    derivatives = [ff for ff in f]\n",
    "    for f_ in f:\n",
    "        for _ in range(n):\n",
    "            grads = torch.autograd.grad(f_.flatten(), wrt, create_graph=True)[0]\n",
    "            f = grads.max(0).values \n",
    "            derivatives.append(f)\n",
    "    return torch.hstack(derivatives)\n",
    "\n",
    "class ZehleTaylor(torch.nn.Module):\n",
    "    def __init__(self, diff_degree, criterion):\n",
    "        super().__init__()\n",
    "        self.diff_degree = diff_degree\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, pred, true, x):\n",
    "        true = higher_order_derivatives(true, x, self.diff_degree)\n",
    "        pred = higher_order_derivatives(pred, x, self.diff_degree)\n",
    "        loss = self.criterion(pred.flatten(), true.flatten())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "hidden_layers = 4\n",
    "output_dim = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-6\n",
    "\n",
    "m = model.NN(input_dim, hidden_dim, hidden_layers)\n",
    "optimizer = torch.optim.SGD(m.parameters(), lr=learning_rate)\n",
    "criterion = ZehleTaylor(3, torch.nn.MSELoss())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    x = X_input.clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    trues = bbobtorch.create_f01(2)(x)\n",
    "    preds = m(x)\n",
    "    \n",
    "    Karloss = criterion(preds, trues, x)\n",
    "\n",
    "    # Backward and optimize\n",
    "    Karloss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {Karloss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate mesh grid for plotting\n",
    "with torch.no_grad():\n",
    "    x = np.linspace(-5.0, 5.0, 100)\n",
    "    y = np.linspace(-5.0, 5.0, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    mesh_samples = np.c_[X.ravel(), Y.ravel()]\n",
    "    mesh_samples_tensor = torch.tensor(mesh_samples, dtype=torch.float32)\n",
    "    mesh_results = m(mesh_samples_tensor).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_meshgrid(X, Y, mesh_results, model='NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collage(sample_f01[0].detach().numpy(), sample_f01[1].detach().numpy(), problem_f01, \"BBOB F24\", \"Phelipe\", X, Y, mesh_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle nn model\n",
    "sys.path.append(\"../\")\n",
    "torch.save(m.state_dict(), \"../models/f01_mse_nn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "- Visually promising results obtained for BBOB F01\n",
    "- Challenge: reengineering functions in a differentiable manner, since a differentiable implementation for F03 and F24 not available\n",
    "\n",
    "-> Simulation not implemented for F03 and F24\n",
    "\n",
    "For future experiments, a more performant implementation of the loss function is required\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CudaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
